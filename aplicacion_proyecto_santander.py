# -*- coding: utf-8 -*-
"""Aplicacion Proyecto Santander.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H-lAxgLEXgway9AKr_if6tWQL-mlr9PA

![logo Santander.jpg](data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEAYABgAAD/4QAsRXhpZgAATU0AKgAAAAgAAQExAAIAAAAKAAAAGgAAAABHcmVlbnNob3QA/9sAQwAHBQUGBQQHBgUGCAcHCAoRCwoJCQoVDxAMERgVGhkYFRgXGx4nIRsdJR0XGCIuIiUoKSssKxogLzMvKjInKisq/9sAQwEHCAgKCQoUCwsUKhwYHCoqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioq/8AAEQgAVwFcAwEiAAIRAQMRAf/EAB8AAAEFAQEBAQEBAAAAAAAAAAABAgMEBQYHCAkKC//EALUQAAIBAwMCBAMFBQQEAAABfQECAwAEEQUSITFBBhNRYQcicRQygZGhCCNCscEVUtHwJDNicoIJChYXGBkaJSYnKCkqNDU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6g4SFhoeIiYqSk5SVlpeYmZqio6Slpqeoqaqys7S1tre4ubrCw8TFxsfIycrS09TV1tfY2drh4uPk5ebn6Onq8fLz9PX29/j5+v/EAB8BAAMBAQEBAQEBAQEAAAAAAAABAgMEBQYHCAkKC//EALURAAIBAgQEAwQHBQQEAAECdwABAgMRBAUhMQYSQVEHYXETIjKBCBRCkaGxwQkjM1LwFWJy0QoWJDThJfEXGBkaJicoKSo1Njc4OTpDREVGR0hJSlNUVVZXWFlaY2RlZmdoaWpzdHV2d3h5eoKDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uLj5OXm5+jp6vLz9PX29/j5+v/aAAwDAQACEQMRAD8A+kaM0UlABmjNFFABmlpKzbDXLTUdTv7G3bMti6pL9SM8Um0iowlJNpaLf8jSzRmiimSGaM0UUAGaM0UUAGaM0UUAGaM0UUAGaM0UUAGaM0UUAGaM0UUAGaM0UUAGaM0UUAGaM0UUAGaM0UUAGaM0UUAGaM0UUAGaM0UUAGaM0UUAGaM0UUAGaM0UUALRQKKACkpaSgAooooAM14ZFrdx4T+KF9cTlvKa6Zbhf7yMc5/Dg120XjQw/FS60i7fbaMiW8WTwsgG7P47iPyrL+LPhnfHHr1onzKBHcgDqOzf0rhxDc4c8N4s+ryiksLiFQxS92tBW+eq/wAvWx6dDKk8KSxMHR1DKw6EGn15z8KPE32zT20S7kzNajdASfvR+n4fyNejV1UqiqQUkeDjsJPBYiVCfT8V0YUUUVocQUVgeK/Ftl4VsVmuQZZpSRFApwX9T7D3rzW5+LuvTyn7JbW0K9gELn8c1z1MRTpu0nqevgsnxmNh7SlH3e7dj2mivFrf4va7C4+029rMO4KlT+leu6RevqWj2l7JF5TXEKyFM525GcU6VeFV2iRjsqxOASlWSs+zLlFFFbnlhRRRQAUUUUAFFFcj4+8X3XhK2s3s7eKdrlnU+aThdoHp9amc1CPNLY3w+HqYmrGjSV5M66isLwdrdx4h8MwajdpGksjOCsfThiO9btEZKSUkTWpSo1JUp7xdn8goooqjIKKKKACiiigAooooAKKK5zxt4kn8L6Et9bQRzSNMsQWQnAyCc8fSplJRjzM2oUZ16saVPd6I6OiuY8CeJLrxRokt7fRRRukxjAizjAAPf6109EZKcVJDxFCeHqypVN1uFFFFUYBRRRQAUUUUAKKKBRQAUlLVe+vIdPsZru6bZDChd29AKBpOTsiSWWOCNpJnWNFGSzHAFYMPjjQ7rXIdJs7r7RczMQDGMqMAnr+FeO+K/GeoeJ7x90jQ2IP7q3U4GPVvU1j6XqEuk6rbX9t/rLeQOB646j8RkV5k8cua0Vofb4bhVug515e+1ol0fS7Ov+KejTab4qGqRAiG9AZXH8MigAj8gD+deg+Etag8Y+ECl6BJKE8i6Q9zjr+I5qfVbKz8deC/3DArcRiWBz1jcdPyPBryXwbrk/hHxZ5d7mOF38i6Q/w84z+B/Sm37GtzfZkRTi8zy32NrVqG3ey/q3qkJ5MvgT4hReeW8u1mDbl6vEf/AKxr2TRvF+ia7hbC+QyH/lk/yt+RrzX4vvC/iOxMRBb7ICxHpuOK4BWKOHRirqchlOCKw9u8NUcI6o9WWWwzrCUsRVbjPltf/NH1JRXmHw58ez3d0mi61J5kjDFvOx5bH8J/xr0+vUpVY1Y80T4PHYGrgazo1f8Ah13PD/i3JK/jUI5OxLVPLHbknP616h4Q0rS7XwxYtp8MTLJCrvJtBLMRzk/Wszx/4JPie2jurFlS/t1ITd0kX+6f6V5nYa74l8C3RtpFkgTdzBOuUb6H/CuFv2FZzmtH1PqacFmuWU8Ph5pThvHa57ld6Lpl9EY7yxt5lPUNGKtxxpDEkcShURQqqOwHQVxHhv4oaZrEqW2op9guW4BY5jY/Xt+Ndle3SWVhPdSfchjLn6AZrvhOE1zRPlcVhsVh5qjXTT6dvl0MzX/FeleG4gdRuMSMMpCgy7fhXHyfGWwDkRaZOy9izgVw+mWd5498aYuZWBnYySv/AM84x2H8q9dtfAHhu1txEumRSYHLycsfxrkjUrVm3T0R9DWweWZZGNPFpzqNXaTsl+Rh2Xxf0meVY7myuYNxA3DDAV22oalb6ZpcuoXbFbeFN7sBkgfT8a8J8Z2Frpfju4tLCFYYI3i2ovQZAJr13xr/AMk+1H/r2H8xTo1qjU1LeJlmOXYSEsNOgmo1fPo7f5mDc/GHRo2xbWl1N7kBRVeH4zae0mJtMuFX1VwcfhXPfC7Q9N1vUL9dUtUuFhjQoH7Ek5/lXoOp/Dzw9e2UkcWnx28m07JYuCp7VFOWJqw54tHTi6OS4Ku8NVpybVtb91fuaug+JNN8R2pn0yfft4dGGGQ+4rhfjR/x6aR/10l/ktct8PLmfTPiDb2ytxK728oHRsZ5/MV1Pxo/49NI/wCukv8AJaJVXVw0m9x0cvjgM7owpu8XqvuZW8GfELSNA8L2+n3qXBmjZy2xMjlia77w34osvFFtNPp6yqkLhG8xcZOM1xHgfwLoet+E7a+1C2aSeRnDMHI6MQK6LVILH4f+C7+bRovKZj8mTnLngGroOrGClJrlsc2aQwFbETpUFL2zlbXa99Sz4h8e6L4dmMFxK09yOsMIyV+p7VzJ+M9lu40qbHvIK5LwL4VPi/WZ5tRkc20OHnbPzSM3QZ/OvV18B+GkhEY0mAqB1I5qYTxFZc0bJGmIw+UZbL2FeMqk+tnZL8UZeh/E/Sta1GCx+zXFvPO21NwBUn610WveILHw3YLd6mzLGz7F2LklsE4/Q14p4diSD4nWsUS7UjvnVVHYDcBXoHxh/wCRUtv+vxf/AEFqdOvN0pSe6DGZThYZhQoU7qM0m9fUguPjJpSMRb2F1IPViFotfjHpcsgF1YXMKn+NSGxVL4ZeGdI1fw7NdajYx3Ey3LIGcdBtHH61p+NPAWjDw3d3mn2i2tzbRmRWj4DAdQR9KlSxLh7RNGlSlktLEvCTpyve17/8H9DtNM1Sz1iwS806dZoX6MOx9D6Gpb29ttPtHub2ZIIYxlnc4Aryn4N3sq6pf2O4mF4RMF7BgcfyP6VW+LGuTXmvro8THyLVQWQfxyNz+gx+da/WbUfaNanF/YbeZvBRl7q1v5f59DpL74v6PbzMlna3F0oON/Cg/TNct4y+INr4q0FLGGzlgkWdZMswIwAR/Wuu8K/DbSrXSoZ9Xt1u7yVA77/upnsBWZ8TfDWkaT4aiutOsYreY3KoXQY4Ibj9BWNRYh0nKTVux6OClk8MbClQhJyT0lfS/wDl8jS+EH/Ip3H/AF9N/wCgium8QeKNM8N2ol1KbazfciUZd/oK5j4REL4QuWPQXTE/98iuDmM/jv4iGKWUrHNMUBz/AKuJfT8B+tUqrp0YKO7MJ5fDGZpiJVnaENWdg/xnsBJhNLnK56lwD+VdL4b8eaR4ll8i3doLrGfIlGCfoe9Oh8C+GYbQQDTYGXGCzcsffNeSeL9H/wCEN8XL/ZkjLGAtxbnPKc9P0onOvRSlNpoeHwuVZlKVDDxlCdrpt3v+Z7nq2qW+i6XPqF6WEEC5baMnrj+tcNP8Y9KQnyLC6kHq2FrV8YXf2/4V3V3/AM9reNz+LLXE/CzQ9N1ubUf7UtEufJEZj3/w53Z/kKutVqe1jCm90c+XYHBrBVcVi4t8krWTt2/VnZeF/iNbeJtYXT4bGWFyjPvZwQAK7SsrT/DOj6VefatPsIoJtu3egwcVq1001NR993Z4mNnhp1b4WLjG2z11FFFAorQ4grmfiJbzXPgTUUtwSwQMQO6ggn9K6amuiyIyOAysMEHuKmceaLj3NsPVdGtGqlflaf3Hy2KK9I8XfC65guJL3w4vnQsSzWucMn+76j2rzq4hltJjFdRPDIpwUkXaa+eqUp0naSP2LBZhh8bDnoyv5dV6o9E+FHib7Levod2/7q4O+3JP3X7r+P8AMe9QfF3R4rTXLbUIV2/bEIkA7svf8q4KGWS3mjmgYpJGwZGHYjpXV+OPFaeJrPSNn+sihLTj0kPGP0z+NbqqpYdwlutjyqmXzo5tDFUfhldS+79dPmcxd3txfzLLdyGR1RYwT2VRgCoKQnHXitLSNA1TXZhHplnJKO8hGEX8a5EpTemrPflKnQheTUYr5Il8KwTXPi7TI7YEyfaFbjsAck/lX0fXH+CfAkHhiM3Ny4uNQkXazgfLGPRf8a7CvcwtGVKHvbs/L+IMxpY7Er2XwxVr9/8AgBVe8sLTULdob63jnjYYKyKCK878Y/EPU/Dfi97O2hiktEiTKyqRuY8khvoRVZPjR+7HmaR8/wDszcfypyxNJNxkzKlkeYTpwrUo3T1Vmv8AgGR8RvBVr4baC+0wstrcOUaInPltjPB9OtdJ4f1O51n4PaiszNJNbRSwBj1YBQR+hx+FcR4i8V6r45vILaK1wiMfKtoAWJY9ya9Z8GeGjofhFNPvADNPuecDoCwxj8BgVy0Up1pOn8Nj3synUw2XUY4x3qqSa6uy7/keffCCSNfFFyjffe1+T8DzXs/avANT0/VPh94qWeEFUjctbzEZSRD/AAn8OMV1qfGeE2wD6S5uCOiy/KT/ADqsPWjSi6dTRoyznLa+YVo4vCLnjJLqtDmPiF/yUi7/AN+L/wBBWvVvGv8AyT3Uf+vYfzFeLeIry+v/ABR9t1a3FrcXJjkEX91eAv6CvaPGp/4t7qP/AF7D+YpUWn7Vr+ty8zg6ccBB7qy+7lOI+DP/ACEtU/64x/zavWm+6fpXknwYYHUtVx/zxj/m1etn7p+ldGD/AIKPG4k/5GVT5fkjwTwl/wAlRtf+v2X/ANmrrfjR/wAemkf9dJf5LXI+EmH/AAtK15/5fZf/AGavQfivo1xqfh6C6tI2keykLuijJ2EYJx7YFcdJN4eaXf8AyPosbONPOMLKbsuVfjdF74Yf8iDZ/wC/J/6Gag+LCs3gdyvRbhC305rgvCHxHk8M6W2nz2guoVctGQ+0qT1Fd/pepD4keEdRjnthaxSMYo/m3cgZB/OuinUhUo+yT1seVjMFiMFmLxtWP7tTvfTZv7zH+DMiHS9SjH+sE6sfoV4/ka9Lr5+0zU9X+HfiSVZ4MH7ksT8LKvYg/wAjXZN8YDdKsGm6PJJdyfKiF8jJ+lLD4iEKfJPRo0zfJ8VicXLEYdc0J2d7rscjoX/JVIP+wg/82rvvjD/yKlt/1+L/AOgtXnvhozD4mWYvFCXH20mVR2Y5JFehfGEgeE7XPH+mL/6C1Y0v4FQ9HHaZthPRfqL8IP8AkUbj/r7b/wBBWun8V/8AIo6p/wBer/yrmPg+QfCNxj/n7b/0Fa6fxX/yKOqf9er/AMq7aX8Beh81mP8AyN5/4l+h5h8Hf+Rnu/8Ar0P/AKEtZHjcGL4lXjS8D7RG3Ppha1vg4wPii7wf+XM/+hLWr8VfCdxcTJrlhE0oCCO5RBkgDo2P0rg5HLCproz6uWIp0c+lGo7c0Uvno/0PT4mV4lZOVZQQfauG+L3/ACJ8P/X2n/oLVy/hz4sS6VpkdlqlqbsQrtSVHw2B0BBqr4u8ZX3i7Qy8OnfZtMt5lLSsclnOQBn8TXRUxNOdJpbtHjYLJMZhcfCc4+7GW91Z9vO77HV/ClS/ge9VepnkA/75FeXaXplzqevrp9pKsNzJI6qzsVGRnjI+leq/B8g+E7jHP+lt/wCgiuT8deE9Q8P6+2s6Ujm1eXzleIZML5ycj0zWNSDdGEui3PUweKjTzTFUG0pT2vtdf8OTf8Ku8Vf8/wDB/wCBDf4VHL8JvEk3Mt1aucYBaVj/AEq9YfGaaO1VNQ05JpVGDJHJtDfh2pZfi5qGo3EVppGmpFJK6oGZjIRk46ClbCNbsPacQRlbkirddLfmdP4rs30/4TXFnKQXgto0YjpkMtc58GP9fq/+7F/7NXXfEHK/DzUt5ywjTcfU71rkPguQZ9Xwf4Yv/Zq3mrYqC8v8zycNJzyPEyfWf/yJ6vRRRXoHyQoooFFABSUtJQAVSv8AR9P1NNt/Zwzj/bQE1dopNJ6MqMpQfNF2ZyFx8MPC9w24WTQn/plIVqEfCjwz/wA8rj/v8a7WisvYUn9lHfHNMdFWVaX3s5ux+H/hqwYNFpkTsOjS/Mf1roIYIreMRwRpGg6Ki4FSUVpGEY/CrHLVxFas71ZuXq7hRRRVGBWu9Os79dt7awzj/pogNZTeCPDbMWbSLbP+5W9RUuMXujaGIrU1aE2vRspWOj6dpgxYWUMHuiAGrtFFNJLRGcpSm+aTuyG6tLe9hMV3BHNGf4XUEVn2/hfRLSYTW+mWySDowjHFa1FDim7tFxrVIR5YyaXqVptOsriXzZ7SGSTAG5kBNTSQxzQmKVFeNhgqwyCKfRRZEc0tNdiC3sbW0Zja20UJbqY0AzU9FFPYTk5O7KsemWMUwmjs4ElByHEYBB9c1aIBGCMiiilZIcpSluzJn8LaHczGWbS7ZnPU+WOa0LWzt7GAQ2cMcMY6Ii4FTUUKKTukXKtUnHllJtepVvtMstSj2X9rFcAdPMQHFQWPh/SdNl82x0+3hk/vKgzWjRRyq97CVWoo8ik7dr6FYabZC488WkImzu8zYN2fXNSXFrBdxhLmFJlByFdcgGpaKLInnle9yK3toLSMpbQpCpOSqKAM090SWNkkUOjDBUjIIp1FMTbbuyvb6fZ2jl7W1hhYjBKIAcVYIBGCMg0UUWsDk5O7Zkz+FtDuZjLNpdsznqfLHNXRp1mLUWwtYRAOkewbfyqzRU8sVsjSVarJJOTdvMit7WC0QpbQpEpOSqLgZqRlV1KsAwPUEdaWiqM223dmRP4V0O4kLzaVasx6nyxVmy0TTNOObKxghPqkYB/Or1FTyxTvY1detKPK5u3qxksMc8TRzxrJG3VWGQajt7K1tCxtbeKEt97y0AzU9FOyM+aSVr6BRRRTJFFFAooAKSiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAFFFFFAH/2Q==)

### Reto: People Analytics. Construcción de un modelo para segmentar un conjunto de líderes que defina las características culturales de nuestra organización e identificar el conjunto de características que deben tener los líderes que nos permitan tener un mayor desempeño alineados con la voluntad de ser un una empresa más diversa, inclusiva y heterogénea.

#### Conjunto de datos: El conjunto de datos será un conjunto de personas junto con sus características demográficas, desempeño, formación y carrera en la compañía, con un histórico de su evolución en 3 años. Entre los campos a incluirán estarán. texto en negrita




# 1.   INTRO: Proceso Análisis Exploratorio

Introducción al EDA (Exploratory Data Analysis).
Notebook por: EQUIPO SANTANDER

# 01 Import
Importamos todas las librerías necesarias para este análisis
"""

#**********************************************************************
# Import packages
#**********************************************************************
import sys
import numpy as np
import os.path
import pandas as pd
import seaborn as sns
sns.set(font_scale=1.5)
sns.set_style("whitegrid")

"""Cargamos el archivo desde nuestra carpeta local

"""

# Subimos el archivo
from google.colab import files
uploaded = files.upload()

# haciendo una asignacion, en el dataframe se actua directamente creando la nueva columna , quedando modificado el DF
df_2020 = pd.read_excel ("Proyect_SPA_2020.xlsx")
df_2020.head(5)

df_2020.describe()

df_2020.info()

df_2021 = pd.read_excel ("Proyect_SPA_2021.xlsx")
df_2021.head(5)

df_2021.describe()

df_2021.info()

df_2022 = pd.read_excel ("Proyect_SPA_2022.xlsx")
df_2022.head(5)

df_2022.describe()

df_2022.info()

"""## Juntar Dataframes
Tenemos tres dataframes.

Los de los años 2020 y 2021 comparten completamente los encabezados de las columnas. Por ello, juntaremos ambos en un dataframe intermedio mediante la función "concat".


"""

df_intermed = pd.concat([df_2020, df_2021])

df_intermed.head()

df_intermed.info ()

"""Vamos a hacer data enginering para transformar los datos de evaluación de 2020 y 2021 a los mismos conceptos que 2022

Vamos a crear una columna nueva en el dataset, darle el nombre que queremos y que esa columna sea la media de las otras dos.
En otros casos solo hay que cambiar el nombre de las columnas
"""

df_20_21=df_intermed.rename( columns={'Support people - Global':'Think Customer – Pienso en el cliente - Global', 'Support people - Autovaloracion': 'Think Customer – Pienso en el cliente - Autovaloracion', 'Support people - Manager': 'Think Customer – Pienso en el cliente - Manager', 'Support people - Colaboradores': 'Think Customer – Pienso en el cliente - Colaboradores',
                                      'Embrace change - Global': 'Embrace Change – Impulso el cambio - Global', 'Embrace change - Autovaloracion': 'Embrace Change – Impulso el cambio - Autovaloracion', 'Embrace change - Manager': 'Embrace Change – Impulso el cambio - Manager', 'Embrace change - Colaboradores': 'Embrace Change – Impulso el cambio - Colaboradores'})

df_20_21.info()

df_20_21

"""Creamos las nuevas columnas unificando criterios al 2022.
Eliminamos las columnas antiguas de las que hemos calculado la media
Ordenamos las columnas

"""

df_20_21['Act Now – Actúo con rapidez - Global']= df_20_21[["Keep promises - Global", "Bring passion - Global"]].mean(axis=1)
df_20_21['Move Together – Trabajo en equipo - Global']= df_20_21[["Actively collaborate - Global", "Show respect - Global"]].mean(axis=1)
df_20_21['Speak Up – Hablo abiertamente - Global']= df_20_21[["Truly listen - Global", "Talk straight - Global"]].mean(axis=1)
df_20_21['Act Now – Actúo con rapidez - Autovaloracion']= df_20_21[["Keep promises - Autovaloracion", "Bring passion - Autovaloracion"]].mean(axis=1)
df_20_21['Move Together – Trabajo en equipo - Autovaloracion']= df_20_21[["Actively collaborate - Autovaloracion", "Show respect - Autovaloracion"]].mean(axis=1)
df_20_21['Speak Up – Hablo abiertamente - Autovaloracion']= df_20_21[["Truly listen - Autovaloracion", "Talk straight - Autovaloracion"]].mean(axis=1)
df_20_21['Act Now – Actúo con rapidez - Manager']= df_20_21[["Keep promises - Manager", "Bring passion - Manager"]].mean(axis=1)
df_20_21['Move Together – Trabajo en equipo - Manager']= df_20_21[["Actively collaborate - Manager", "Show respect - Manager"]].mean(axis=1)
df_20_21['Speak Up – Hablo abiertamente - Manager']= df_20_21[["Truly listen - Manager", "Talk straight - Manager"]].mean(axis=1)
df_20_21['Act Now – Actúo con rapidez - Colaboradores']= df_20_21[["Keep promises - Colaboradores", "Bring passion - Colaboradores"]].mean(axis=1)
df_20_21['Move Together – Trabajo en equipo - Colaboradores']= df_20_21[["Actively collaborate - Colaboradores", "Show respect - Colaboradores"]].mean(axis=1)
df_20_21['Speak Up – Hablo abiertamente - Colaboradores']= df_20_21[["Truly listen - Colaboradores", "Talk straight - Colaboradores"]].mean(axis=1)

df_20_21

df_20_21.columns

df_20_21=df_20_21.drop(columns=['Show respect - Global',
       'Truly listen - Global', 'Talk straight - Global',
       'Keep promises - Global', 'Actively collaborate - Global',
       'Bring passion - Global',
       'Show respect - Autovaloracion',
       'Truly listen - Autovaloracion', 'Talk straight - Autovaloracion',
       'Keep promises - Autovaloracion',
       'Actively collaborate - Autovaloracion',
       'Bring passion - Autovaloracion',
        'Show respect - Manager',
       'Truly listen - Manager', 'Talk straight - Manager',
       'Keep promises - Manager', 'Actively collaborate - Manager',
       'Bring passion - Manager',
        'Show respect - Colaboradores',
       'Truly listen - Colaboradores', 'Talk straight - Colaboradores',
       'Keep promises - Colaboradores', 'Actively collaborate - Colaboradores',
       'Bring passion - Colaboradores'
       ])

df_20_21.info()

df_2022.columns

df_2022.describe

orden_columnas=['ID', 'Gender', 'Country of Origin', 'Age', 'Tenure', 'Job Family',
       'Position Level 1', 'Position Level 2', 'Position Level 3',
       'Position Level 4', 'Position Level 5', 'Position Level 6',
       'Position Level 7', 'Position Level 8', 'Management Level',
       'Corporate Segment', 'Negocio/ No Negocio', 'mar_status', 'n_children',
       'field_study', 'spain_of_control', 'pct_women', 'pct_below40',
       'pct_above60', 'avg_age_sub', 'avg_ten_sub', 'pct_corp_seg', 'pct_STEM',
       'pct_mngt_lvl', 'what_performance_rating_h', 'what_performance_label_h',
       'what_performance_rating_f', 'what_performance_label_f',
       'how_performance_rating', 'how_performance_label',
       'risk_performance_rating', 'risk_performance_label',
       'overall_manager_rating', 'overall_manager_label',
       'emp_what_perf_rating', 'emp_what_perf_label', 'emp_how_perf_rating',
       'emp_how_perf_label', 'emp_rsk_perf_rating', 'emp_rsk_perf_label',
       'overall_employee_rating', 'overall_employee_label', 'year_performance',
       'Valoracion 360 Global',
       'Think Customer – Pienso en el cliente - Global',
       'Embrace Change – Impulso el cambio - Global',
       'Act Now – Actúo con rapidez - Global',
       'Move Together – Trabajo en equipo - Global',
       'Speak Up – Hablo abiertamente - Global',
       'Valoracion 360 Autovaloracion',
       'Think Customer – Pienso en el cliente - Autovaloracion',
       'Embrace Change – Impulso el cambio - Autovaloracion',
       'Act Now – Actúo con rapidez - Autovaloracion',
       'Move Together – Trabajo en equipo - Autovaloracion',
       'Speak Up – Hablo abiertamente - Autovaloracion',
       'Valoracion 360 Manager',
       'Think Customer – Pienso en el cliente - Manager',
       'Embrace Change – Impulso el cambio - Manager',
       'Act Now – Actúo con rapidez - Manager',
       'Move Together – Trabajo en equipo - Manager',
       'Speak Up – Hablo abiertamente - Manager',
       'Valoracion 360 Colaboradores',
       'Think Customer – Pienso en el cliente - Colaboradores',
       'Embrace Change – Impulso el cambio - Colaboradores',
       'Act Now – Actúo con rapidez - Colaboradores',
       'Move Together – Trabajo en equipo - Colaboradores',
       'Speak Up – Hablo abiertamente - Colaboradores']

"""# Nueva sección"""

df_20_21=df_20_21[orden_columnas]

df_20_21.head(150)

df_completo = pd.concat([df_20_21, df_2022])

# Reiniciar el índice con una nueva secuencia numérica
df_completo.reset_index(drop=True, inplace=True)

df_completo.info()

df_completo.shape

df_completo.describe()

#saco % de la columna Job Family (Javier)
job_family_counts = df_completo['Job Family'].value_counts()
job_family_percentages = (job_family_counts / len(df_completo)) * 100
print(job_family_percentages)

# Obtener los valores únicos de la columna 'year_performance' (Javier)
year_performance_values = df_completo['year_performance'].unique()
print(year_performance_values)

# Contar el número de filas por cada valor de 'year_performance' (Javier)
count_by_year_performance = df_completo['year_performance'].value_counts()

print(count_by_year_performance)

# Contar los valores nulos en 'Valoracion 360 Global' y agrupar por 'year_performance' (Javier)
null_counts_by_year = df_completo['Valoracion 360 Global'].isnull().groupby(df_completo['year_performance']).sum()

print(null_counts_by_year)

# Contar los valores nulos en 'Valoracion 360 Manager' y agrupar por 'year_performance' (Javier)
null_counts_by_year = df_completo['Valoracion 360 Manager'].isnull().groupby(df_completo['year_performance']).sum()

print(null_counts_by_year)

# Contar los valores nulos en 'Negocio / No Negocio' y agrupar por 'year_performance' (Javier) LOS PENDIENTES LOS COMPLETAMOS NOSOTROS
null_counts_by_year = df_completo['Negocio/ No Negocio'].isnull().groupby(df_completo['year_performance']).sum()

print(null_counts_by_year)

# Calcular el conteo y porcentaje de valores en 'Negocio/ No Negocio' agrupados por 'year_performance'
grouped_counts = df_completo.groupby('year_performance')['Negocio/ No Negocio'].value_counts()
grouped_percentages = grouped_counts.groupby(level=0).apply(lambda x: (x / x.sum()) * 100)

print(grouped_counts)
print(grouped_percentages)

# Contar los valores nulos en 'field_study' y agrupar por 'year_performance' (Javier)
null_counts_by_year = df_completo['field_study'].isnull().groupby(df_completo['year_performance']).sum()

print(null_counts_by_year)

# Contar filas con todos los campos completos (Javier)
complete_rows_count = df_completo.dropna().shape[0]

print("Número de filas con todos los campos completos:", complete_rows_count)

# Contar el número de campos incompletos por fila (Javier)
incomplete_fields_count = df_completo.isnull().sum(axis=1)

# Agrupar las filas por el número de campos incompletos y contar cuántas filas en cada grupo
grouped_counts = incomplete_fields_count.groupby(pd.cut(incomplete_fields_count, bins=[0, 5, 10, 15, 20, float('inf')], right=False)).count()

print(grouped_counts)

# Contar el número de campos incompletos por fila (Javier)
incomplete_fields_count = df_completo.isnull().sum(axis=1)

# Crear una nueva columna 'Incomplete Fields Count'
df_completo['Incomplete Fields Count'] = incomplete_fields_count

# Agrupar por 'year_performance' y 'Incomplete Fields Count', y contar cuántas filas en cada grupo
grouped_counts = df_completo.groupby(['year_performance', pd.cut(df_completo['Incomplete Fields Count'], bins=[0, 5, 10, 15, 20, float('inf')], right=False)])['Incomplete Fields Count'].count()

# Imprimir los resultados
print(grouped_counts)

df_completo.shape

# Filtrar el dataframe y contar campos vacíos por columna por el valor "2022" en la columna "year_performance" (Javier)
filtered_df = df_completo[df_completo['year_performance'] == 2022]

empty_fields_count = filtered_df.isnull().sum()

for column, count in empty_fields_count.items():
    print(f"Columna: {column}, Campos vacíos: {count}")

# Añadir una columna al df que sume cuantas "Position Level" tiene cada ID, la llamamos num_position_levels (Javier)
position_level_columns = ['Position Level 1', 'Position Level 2', 'Position Level 3',
                          'Position Level 4', 'Position Level 5', 'Position Level 6',
                          'Position Level 7', 'Position Level 8']


df_completo['num_position_levels'] = df_completo[position_level_columns].notnull().sum(axis=1)

print(df_completo['num_position_levels'])

df_completo.shape

df_completo

"""# NUEVO DATA FRAME PARA RELLENAR CAMPOS NULOS"""

## CREAR UN NUEVO DF PARA RELLENAR CAMPOS INCOMPLETOS
    # Crear una copia del DataFrame original y lo llamamos df_compelto_01 (Javier)
df_completo_01 = df_completo.copy()

  # Llenar con ceros los campos vacíos en la columna 'n_children' (visto con tutor)
df_completo_01['n_children'] = df_completo_01['n_children'].fillna(0)

df_completo_01

# Usando str.replace() para eliminar la cadena de texto (NEW) de los campos en la columna 'Job Family'
df_completo_01['Job Family'] = df_completo_01['Job Family'].str.replace('(NEW)', '')
unique_job_families = df_completo_01['Job Family'].unique()
print("Campos únicos en la columna 'Job Family':")
print(unique_job_families)

# Eliminar la cadena de texto "()" de todos los campos de la columna 'Job Family'
df_completo_01['Job Family'] = df_completo_01['Job Family'].str.replace(r'\(\)', '')

# Obtener el nuevo recuento de campos únicos en la columna 'Job Family'
unique_job_family_after_remove = df_completo_01['Job Family'].nunique()

# Mostrar el resultado
print("Cantidad de campos únicos en 'Job Family' después de eliminar '()':", unique_job_family_after_remove)

# Contar cuántos campos de la columna 'Job Family' incluyen el texto "()"
num_job_family_with_empty = df_completo_01['Job Family'].str.contains(r'\(\)').sum()

# Mostrar el resultado
print("Cantidad de campos en 'Job Family' que incluyen '()':", num_job_family_with_empty)

#quizás así lo hayamos solucionado (ya solo hay 175 tipos de Job Family), pero me quedaría más tranquilo si alguien con mejor crriterio en Python lo revisara

#siguiendo las instrucciones del tutor, vamos a rellenar las columnas Overall_performance_rating con el valor 2 y Overall_performance_label con Achieved
#el sentido de esto es que es el KPI más importante, es la variable objetivo a utilizar y se asume que los nulos son los valores mencionados
# Contar cuántos campos vacíos en 'overall_manager_rating'
num_empty_overall_manager_rating = df_completo_01['overall_manager_rating'].isna().sum()

# Contar cuántos campos vacíos en 'overall_manager_label'
num_empty_overall_manager_label = df_completo_01['overall_manager_label'].isna().sum()

# Mostrar los resultados
print("Cantidad de campos vacíos en 'overall_manager_rating':", num_empty_overall_manager_rating)
print("Cantidad de campos vacíos en 'overall_manager_label':", num_empty_overall_manager_label)

#vemos que hay un problema y que puede haber labels sin rating y viceversa, los buscamos
# Contar cuántas filas con valor en 'overall_manager_rating', pero sin valor en 'overall_manager_label'
num_rows_only_overall_manager_rating = df_completo_01[df_completo_01['overall_manager_rating'].notnull() & df_completo_01['overall_manager_label'].isnull()].shape[0]

# Contar cuántas filas con valor en 'overall_manager_label', pero sin valor en 'overall_manager_rating'
num_rows_only_overall_manager_label = df_completo_01[df_completo_01['overall_manager_label'].notnull() & df_completo_01['overall_manager_rating'].isnull()].shape[0]

# Mostrar los resultados
print("Cantidad de filas con valor en 'overall_manager_rating', pero sin valor en 'overall_manager_label':", num_rows_only_overall_manager_rating)
print("Cantidad de filas con valor en 'overall_manager_label', pero sin valor en 'overall_manager_rating':", num_rows_only_overall_manager_label)

#Vamos a buscar esos 41 IDs que tienen valores en label pero no en rating y vemos si podemos asignar esos ratings
# Filtrar el DataFrame para obtener las filas con valor en 'overall_manager_label', pero sin valor en 'overall_manager_rating'
filtered_df = df_completo_01[df_completo_01['overall_manager_label'].notnull() & df_completo_01['overall_manager_rating'].isnull()]

# Obtener los valores de los campos en 'overall_manager_label' que cumplen con la condición
values_only_overall_manager_label = filtered_df['overall_manager_label']

# Mostrar los resultados
print("Valores en 'overall_manager_label' que no tienen valor en 'overall_manager_rating':")
print(values_only_overall_manager_label)

#resulta que son labels con valor 0, que no es un valor en el rango de los labels que deberían de aparecer. Aquí creo que hay que preguntar al tutor si los tomamos como un error o como un Not Achieved
# una vez nos responda, podremos terminar de asignar todos los ratings y labels a los campos vacíos

#vamos a explorar cuantos ID se repiten cada año
#primero vamos a ver cuantos hay por año
unique_id_by_year_performance = df_completo_01.groupby('year_performance')['ID'].nunique()

print("Valores únicos de 'ID' filtrados por 'year_performance':")
print(unique_id_by_year_performance)

#Vamos a empezar con los IDs del 2020 que se repiten en 2021 y en 2022
# Filtrar los valores de 'ID' correspondientes al año 2020
ids_2020 = df_completo_01[df_completo_01['year_performance'] == 2020]['ID']

# Filtrar los valores de 'ID' correspondientes al año 2021
ids_2021 = df_completo_01[df_completo_01['year_performance'] == 2021]['ID']

# Filtrar los valores de 'ID' correspondientes al año 2022
ids_2022 = df_completo_01[df_completo_01['year_performance'] == 2022]['ID']

# Contar cuántos valores de 'ID' de 2020 se repiten en 2021
num_ids_repeated_2021 = ids_2020.isin(ids_2021).sum()

# Contar cuántos valores de 'ID' de 2020 se repiten en 2022
num_ids_repeated_2022 = ids_2020.isin(ids_2022).sum()

# Mostrar los resultados
print("Cantidad de valores de 'ID' de 2020 que se repiten en 2021:", num_ids_repeated_2021)
print("Cantidad de valores de 'ID' de 2020 que se repiten en 2022:", num_ids_repeated_2022)

#Ahora con los de 2021 que se repiten en 2022
# Filtrar los valores de 'ID' correspondientes al año 2021
ids_2021 = df_completo_01[df_completo_01['year_performance'] == 2021]['ID']

# Filtrar los valores de 'ID' correspondientes al año 2022
ids_2022 = df_completo_01[df_completo_01['year_performance'] == 2022]['ID']

# Contar cuántos valores de 'ID' de 2021 se repiten en 2022
num_ids_repeated_2021_to_2022 = ids_2021.isin(ids_2022).sum()

# Mostrar el resultado
print("Cantidad de valores de 'ID' de 2021 que se repiten en 2022:", num_ids_repeated_2021_to_2022)

#se repiten la gran mayoría de los valores a lo largo de los tres años (del 20 al 21 solo hay 2 que no se repiten y del 20 al 22 son 17. Del 21 al 22)
#Ahora vamos a ver si los valores que se repiten, tiene cambios en su trayectoria (tenure y Job Family)
# Filtrar los datos correspondientes al año 2020 y 2021
data_2020 = df_completo_01[df_completo_01['year_performance'] == 2020]
data_2021 = df_completo_01[df_completo_01['year_performance'] == 2021]

# Encontrar los 'ID' que se repiten del 2020 al 2021
ids_repeated_2020_to_2021 = data_2020[data_2020['ID'].isin(data_2021['ID'])]['ID'].unique()

# Verificar si hay cambios en el valor de 'tenure' para los 'ID' que se repiten
changes_in_tenure = False
for id_ in ids_repeated_2020_to_2021:
    tenure_2020 = data_2020[data_2020['ID'] == id_]['Tenure'].iloc[0]
    tenure_2021 = data_2021[data_2021['ID'] == id_]['Tenure'].iloc[0]

    if tenure_2020 != tenure_2021:
        changes_in_tenure = True
        break

# Mostrar el resultado
if changes_in_tenure:
    print("Hay cambios en el valor de 'tenure' para los 'ID' que se repiten del 2020 al 2021.")
else:
    print("No hay cambios en el valor de 'tenure' para los 'ID' que se repiten del 2020 al 2021.")

#Hacemos lo mismo para ver los que repiten de 2021 a 2022
# Filtrar los datos correspondientes al año 2021 y 2022
data_2021 = df_completo_01[df_completo_01['year_performance'] == 2021]
data_2022 = df_completo_01[df_completo_01['year_performance'] == 2022]

# Encontrar los 'ID' que se repiten del 2021 al 2022
ids_repeated_2021_to_2022 = data_2021[data_2021['ID'].isin(data_2022['ID'])]['ID'].unique()

# Verificar si hay cambios en el valor de 'Tenure' para los 'ID' que se repiten
changes_in_tenure = False
for id_ in ids_repeated_2021_to_2022:
    tenure_2021 = data_2021[data_2021['ID'] == id_]['Tenure'].iloc[0]
    tenure_2022 = data_2022[data_2022['ID'] == id_]['Tenure'].iloc[0]

    if tenure_2021 != tenure_2022:
        changes_in_tenure = True
        break

# Mostrar el resultado
if changes_in_tenure:
    print("Hay cambios en el valor de 'Tenure' para los 'ID' que se repiten del 2021 al 2022.")
else:
    print("No hay cambios en el valor de 'Tenure' para los 'ID' que se repiten del 2021 al 2022.")

#Las fechas de 'tenure' son las mismas, ahora vamos a ver si algún ID cambió de Job Family durante los 3 años
# Encontrar los 'ID' que se repiten en al menos dos años
repeated_ids = df_completo_01['ID'].duplicated(keep=False)

# Filtrar los datos para incluir solo los 'ID' que se repiten en dos o más años
data_repeated_ids = df_completo_01[repeated_ids]

# Contar cuántos 'ID' han tenido cambios en 'Job Family'
count_changes_in_job_family = 0

# Iterar sobre cada 'ID' repetido
for id_ in data_repeated_ids['ID'].unique():
    data_id = data_repeated_ids[data_repeated_ids['ID'] == id_]
    job_family_values = data_id['Job Family'].unique()

    # Si hay más de un valor único de 'Job Family' para el 'ID', entonces hubo un cambio
    if len(job_family_values) > 1:
        count_changes_in_job_family += 1

# Mostrar el conteo de 'ID' que han tenido cambios en 'Job Family'
print("Cantidad de 'ID' que han tenido cambios en 'Job Family':", count_changes_in_job_family)

#¿Puede ser que tome todos los job Family de 2022 como nuevos o como cambio? Si no, no se explicarme una cifra tan alta: Luis respondió esto: Las familias es un concepto que antes era local(por país)
#y se ha ido armonizando a un catalogo común, que se compone de 54 familias.

# Rellena los campos vacíos en la columna 'overall_manager_rating' con el valor 2
df_completo_01['overall_manager_rating'].fillna(2, inplace=True)
# Rellena los campos vacíos en la columna 'overall_manager_label' con el valor 'Achieved'
df_completo_01['overall_manager_label'].fillna('Achieved', inplace=True)

# Rellena los campos con valor 0 en la columna 'overall_manager_label' con el valor 'Achieved'
df_completo_01['overall_manager_label'] = df_completo_01['overall_manager_label'].map(lambda x: 'Achieved' if x == 0 else x)
# Cantidad de valores únicos en la columna 'overall_manager_label'
cantidad_valores_unicos = df_completo_01['overall_manager_label'].nunique()

# Lista de valores únicos en la columna 'overall_manager_label'
valores_unicos = df_completo_01['overall_manager_label'].unique()

print("Cantidad de valores únicos en la columna 'overall_manager_label':", cantidad_valores_unicos)
print("Lista de valores únicos en la columna 'overall_manager_label':", valores_unicos)

# Correlaciones entre todas las columnas
correlacion_df_completo = df_completo_01.corr()
print(correlacion_df_completo)

#Correlación entre columnas de intere#Correlación entre columnas de interes
columnas_para_correlacion = ['overall_manager_label', 'overall_manager_rating', 'spain_of_control', 'Valoracion 360 Global', 'Valoracion 360 Autovaloracion', 'Valoracion 360 Colaboradores']
df_columnas_correlacion = df_completo_01[columnas_para_correlacion]

#Correlación entre la columna de interes con el resto del nuevo DataFrame
column_interes = 'overall_manager_label'
correlations_with_column = df_columnas_correlacion.corr(method='pearson')
print(correlations_with_column)

#no vemos que esto aporte nada
#column_interes1 = 'overall_manager_rating'
#correlations_with_column1 = df_columnas_correlacion.corr(method='pearson')
#print(correlations_with_column1)

# Cantidad de valores únicos en la columna 'mar_status'
cantidad_valores_unicos = df_completo_01['mar_status'].nunique()

# Lista de valores únicos en la columna 'mar_status'
valores_unicos = df_completo_01['mar_status'].unique()

print("Cantidad de valores únicos en la columna 'mar_status':", cantidad_valores_unicos)
print("Lista de valores únicos en la columna 'mar_status':", valores_unicos)

# Cantidad de valores únicos en la columna 'field_study'
cantidad_valores_unicos = df_completo_01['field_study'].nunique()

# Lista de valores únicos en la columna 'field_study'
valores_unicos = df_completo_01['field_study'].unique()

print("Cantidad de valores únicos en la columna 'field_study':", cantidad_valores_unicos)
print("Lista de valores únicos en la columna 'field_study':", valores_unicos)

# Cantidad de valores únicos en la columna 'Management Level'
cantidad_valores_unicos = df_completo_01['Management Level'].nunique()

# Lista de valores únicos en la columna 'Management Level'
valores_unicos = df_completo_01['Management Level'].unique()

print("Cantidad de valores únicos en la columna 'Management Level':", cantidad_valores_unicos)
print("Lista de valores únicos en la columna 'Management Level':", valores_unicos)

# Cantidad de valores únicos en la columna 'Job Family'
cantidad_valores_unicos = df_completo_01['Job Family'].nunique()

# Lista de valores únicos en la columna 'Job Family'
valores_unicos = df_completo_01['Job Family'].unique()

print("Cantidad de valores únicos en la columna 'Job Family':", cantidad_valores_unicos)
print("Lista de valores únicos en la columna 'Job Family':", valores_unicos)

"""Para unificar el valor de "Job Family" de cada "ID" al valor de "Job Family" cuando "year_performance" = 2022 según la decisión tomada de tener sólo en cuenta el valor actual vigente del "Job Family":

1. Primero, vamos a filtrar el DataFrame para obtener solo las filas que tienen "year_performance" igual a 2022.
2. Luego, vamos a crear un diccionario que mapee cada "ID" al valor de "Job Family" correspondiente para el año 2022.
3. Finalmente, actualizaremos el DataFrame utilizando el diccionario creado para unificar el valor de "Job Family" de cada "ID".

Después de ejecutar este código, el DataFrame `df_completo_01` tendrá la columna "Job Family" actualizada con los valores unificados de 2022 para cada "ID".
"""

df_completo_01.shape

df_completo_01.columns

# Paso 1: Filtrar las filas con "year_performance" igual a 2022
df_2022 = df_completo_01[df_completo_01['year_performance'] == 2022]

# Paso 2: Crear un diccionario que mapee cada "ID" al valor de "Job Family" de 2022
id_to_job_family_2022 = df_2022.set_index('ID')['Job Family'].to_dict()

# Paso 3: Actualizar el DataFrame original con los valores unificados de "Job Family"
df_completo_01['Job Family'] = df_completo_01['ID'].map(id_to_job_family_2022)

# Imprimir los valores únicos después de la actualización
cantidad_valores_unicos = df_completo_01['Job Family'].nunique()
valores_unicos = df_completo_01['Job Family'].unique()

print("Cantidad de valores únicos en la columna 'Job Family':", cantidad_valores_unicos)
print("Lista de valores únicos en la columna 'Job Family':", valores_unicos)

#Filtrar las entradas con "ID" igual a "AA001" para comprobar que el Job Family es igual en los tres años
filtro = df_completo_01["ID"] == "AA001"
entradas_con_id_concreto = df_completo_01[filtro]
print(entradas_con_id_concreto)

"""La función revisa cuál es el valor de "Job Family" y lo incluye en una nueva columna que indica el "Job Family Group" correspondiente"""

#Definir la función para obtener el grupo de "Job Family"
def get_job_family_group(job_family):
    if job_family in ['Distribution Management', 'Customer S&S: F2F', 'Customer S&S: Remote', 'Customer S&S: Specialized products', 'Commercial & Business Banker', 'Banker', 'Sales', 'Transaction Banking', 'Debt Finance', 'M&A', 'Investment & Asset Management']:
        return "Customer Facing"
    elif job_family in ['Research and Business Intelligence', 'Product & Service Value Proposition Management', 'Business Development & Partnerships', 'Trading', 'Middle Office']:
        return "Non-customer Facing"
    elif job_family in ['Audit', 'Compliance', 'Recovery & Collections', 'EWRM', 'Credit Risk', 'Market & Structural Risk', 'Non-financial Risk']:
        return "Internal Control"
    elif job_family in ['Accounting', 'Analysis & Control', 'Financial Management', 'Capital']:
        return "Management Control"
    elif job_family in ['Software Engineering', 'Systems Infrastructure & Communication Platforms']:
        return "IT Delivery"
    elif job_family in ['Product & IT Project Management']:
        return "Product & IT Project Management"
    elif job_family in ['Solution Application, Infrastructure & Data Architecture', 'Technology Risk & Cybersecurity']:
        return "IT Enterprise"
    elif job_family in ['Service & Delivery Management', 'End User Technologies']:
        return "Service Support"
    elif job_family in ['Data Analytics & Models', 'Data Management & Governance']:
        return "Data"
    elif job_family in ['Admin & Support', 'Properties & Gral. Services', 'Procurement & Third Party Cost Mgmt.']:
        return "General Services"
    elif job_family in ['HR Specialist', 'HR Management']:
        return "Human Resources"
    elif job_family in ['Operations']:
        return "Operations"
    elif job_family in ['Investor Relations', 'Supervisory & Public Stakeholder Management', 'Communication', 'Marketing']:
        return "Skateholder Management & Marketing"
    elif job_family in ['Tax', 'Legal', 'Governance']:
        return "Counsel & Governance"
    elif job_family in ['Top Management', 'Strategy & Corporate Development', 'Project', 'Early Careers']:
        return "Other Staff & Support"
    else:
        return None

#Agregar una nueva columna "Job Family Group" al DataFrame
df_completo_01["Job Family Group"] = df_completo_01["Job Family"].apply(get_job_family_group)

"""La función revisa cuál es el valor de "Job Family Group" y lo incluye en una columna nueva que indica el "Job Family Category" correspondiente"""

#Definir la función para obtener "Job Family Category"
def get_job_family_category(job_family_group):
    if job_family_group in ['Customer Facing', 'Non-customer Facing']:
        return "Business"
    elif job_family_group in ['Internal Control', 'Management Control']:
        return "Control & Oversight"
    elif job_family_group in ['T Delivery', 'Product & IT Project Management', 'IT Enterprise', 'Service Support', 'Data', 'General Services', 'Human Resources', 'Operations', 'Skateholder Management & Marketing', 'Counsel & Governance', 'Other Staff & Support']:
        return "Staff & Support"
    else:
        return None

#Agregar una nueva columna "Job Family Group" al DataFrame
df_completo_01["Job Family Category"] = df_completo_01["Job Family Group"].apply(get_job_family_category)

df_completo_01.shape

"""##Convertimos en categórica la variable "Gender"
"""

#Codificación one-hot para 'Gender'
df_completo_01_encoded = pd.get_dummies(df_completo_01, columns=["Gender"],prefix="")

df_completo_01_encoded.shape

df_completo_01_encoded.head(3)

"""##Para convertir las variables categóricas "Job Family Group" y "Job Family Category" en variables cuantitativas, puedes usar la técnica de codificación one-hot o de etiquetado numérico:

1.   Codificación one-hot: La codificación one-hot crea una columna binaria para cada categoría en la variable original. Cada columna binaria representa la presencia o ausencia de la categoría en la observación. Se puede hacer esto usando la función get_dummies de pandas para la columna "Job Family Group" y la columna "Job Family Category":
"""

#Codificación one-hot para 'Job Family Group'
df_completo_01_ohe = pd.get_dummies(df_completo_01_encoded, columns=["Job Family Group"], prefix="JFG")

df_completo_01_ohe.shape

# Codificación one-hot para 'Job Family Category'
df_completo_01_ohe = pd.get_dummies(df_completo_01_ohe, columns=["Job Family Category"], prefix="JFC")

df_completo_01_ohe.shape

df_completo_01_ohe.head(3)

"""

2.   Etiquetado numérico: El etiquetado numérico asigna un número entero a cada categoría en función de su posición en la lista de categorías. Se puede usar el método factorize de pandas para realizar el etiquetado numérico:

"""

df_completo_01_et_num=df_completo_01.copy()

#Etiquetado numérico para 'Job Family Group'
df_completo_01_et_num["JFG_numerical"] = pd.factorize(df_completo_01_encoded["Job Family Group"])[0]

df_completo_01_et_num.shape

# Etiquetado numérico para 'Job Family Category'
df_completo_01_et_num["JFC_numerical"] = pd.factorize(df_completo_01_et_num["Job Family Category"])[0]

df_completo_01_et_num.shape

df_completo_01_et_num.head(3)

columns_to_normalize=['n_children','spain_of_control','pct_women','pct_below40','pct_above60','avg_age_sub','avg_ten_sub','pct_corp_seg','pct_STEM',
         'pct_mngt_lvl','what_performance_rating_h','what_performance_rating_f','how_performance_rating',
         'risk_performance_rating','overall_manager_rating','emp_what_perf_rating','emp_how_perf_rating','emp_rsk_perf_rating','overall_employee_rating','Valoracion 360 Global',
         'Think Customer – Pienso en el cliente - Global','Embrace Change – Impulso el cambio - Global','Act Now – Actúo con rapidez - Global','Move Together – Trabajo en equipo - Global',
         'Speak Up – Hablo abiertamente - Global','Valoracion 360 Autovaloracion','Think Customer – Pienso en el cliente - Autovaloracion','Embrace Change – Impulso el cambio - Autovaloracion',
         'Act Now – Actúo con rapidez - Autovaloracion','Move Together – Trabajo en equipo - Autovaloracion','Speak Up – Hablo abiertamente - Autovaloracion','Valoracion 360 Manager',
         'Think Customer – Pienso en el cliente - Manager','Embrace Change – Impulso el cambio - Manager','Act Now – Actúo con rapidez - Manager','Move Together – Trabajo en equipo - Manager',
         'Speak Up – Hablo abiertamente - Manager','Valoracion 360 Colaboradores','Think Customer – Pienso en el cliente - Colaboradores','Embrace Change – Impulso el cambio - Colaboradores',
         'Act Now – Actúo con rapidez - Colaboradores','Move Together – Trabajo en equipo - Colaboradores','Speak Up – Hablo abiertamente - Colaboradores']

data_to_normalize = df_completo_01_ohe[columns_to_normalize]

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaled_df = scaler.fit_transform(data_to_normalize)

scaled_df.shape

print(columns_to_normalize)

data_to_normalize.shape

df_completo_01_ohe.shape

df_completo_01_ohe_scaled = pd.DataFrame(scaled_df, columns=columns_to_normalize)

df_completo_01_ohe.head(3)

df_completo_01_ohe_scaled.head(10)



"""**Random forest de regresión**"""

df_completo_01_ohe_scaled. info

df_completo_01_ohe_scaled = df_completo_01_ohe_scaled.fillna(0)

##Dividimos nuestro dataset en dos dataset: el primero tendrá un 90% de los datos y es el que usaremos para realizar el random forest y el segundo tendrá el 10% de los datos y lo##
##utilizaremos como datos de validación##

total_filas = len(df_completo_01_ohe_scaled)
filas_10porciento = int(6616 * 0.10)
filas_90porciento = total_filas - filas_10porciento
df_10porciento = df_completo_01_ohe_scaled.sample(n=filas_10porciento, random_state=42)
df_90porciento = df_completo_01_ohe_scaled.drop(df_10porciento.index)

# Importa las bibliotecas necesarias
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

# Supongamos que tu objetivo es predecir la columna 'target' (tu variable de respuesta)
# y que las características se encuentran en las columnas restantes de tu DataFrame.

# Divide tus datos en conjuntos de entrenamiento y prueba
X = df_90porciento.drop('Valoracion 360 Global', axis=1)
y = df_90porciento['Valoracion 360 Global']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Crea el modelo de Random Forest de regresión
rf_regressor = RandomForestRegressor(n_estimators=50,max_depth=10,random_state=42)

# Entrena el modelo en los datos de entrenamiento
rf_regressor.fit(X_train, y_train)

# Realiza predicciones en el conjunto de prueba
y_pred = rf_regressor.predict(X_test)

# Evalúa el rendimiento del modelo (por ejemplo, usando el error cuadrático medio)
mse = mean_squared_error(y_test, y_pred)
print(f"Error Cuadrático Medio (MSE): {mse}")
print(f'R-squared (R^2): {r2}')

# También puedes analizar la importancia de las características si lo deseas
feature_importances = rf_regressor.feature_importances_
print("Importancia de las características:")
for feature, importance in zip(X.columns, feature_importances):
    print(f"{feature}: {importance}")

r2 = r2_score(y_test, y_pred)
print(f'R-squared: {r2}')

from sklearn.model_selection import cross_val_score

X = df_10porciento.drop('Valoracion 360 Global', axis=1)
y = df_10porciento['Valoracion 360 Global']

rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)

#Utilizamos una cv de 5#
scores = cross_val_score(rf_regressor, X, y, cv=5, scoring='neg_mean_squared_error')

# Los resultados de cross_val_score son negativos, así que los convertiremos en positivos#
rmse_scores = np.sqrt(-scores)

# Calculamos el promedio y la desviación estándar del RMSE#
mean_rmse = rmse_scores.mean()
std_rmse = rmse_scores.std()

print(f'RMSE promedio: {mean_rmse}')
print(f'Desviación estándar del RMSE: {std_rmse}')

"""Serializamos ahora nuestro modelo para posteriomente poder productivizarlo. Para ello utilizaremos joblib:"""

import pickle
import joblib

# Save the model to a file
model_filename = "modelo_rfregression.grupoSantander.pkl"
joblib.dump(rf_regressor, model_filename)

from google.colab import files
files.download(model_filename)

"""**PRODUCTIVIZACIÓN DEL MODELO**
Inferimos que el modelo va a ser utilizado de forma manual, por lo que la herramienta elegida es Streamlit.

"""

import streamlit as st
import joblib

model = RandomForestRegressor(n_estimators=50)
X = df_10porciento.drop('Valoracion 360 Global', axis=1)
y = df_10porciento['Valoracion 360 Global']
model.fit(X, y)

##Importamos nuestro modelo##
model = joblib.load('modelo_rfregression.grupoSantander.pkl')

# Set the title and subtitle
st.title("Forecast of candidates for Manager positions")
st.subheader("Select the value in the different options to obtain an assessment of the candidate")

# Create input fields for each feature
feature1 = st.slider("Think Customer - Pienso en el cliente - Global", 1.00, 2.00, 3.00, 4.00)
feature2 = st.slider("Embrace Change - Impulso el cambio - Global", 1.00, 2.00, 3.00, 4.00)
feature3 = st.slider("Act Now - Actúo con rapidez - Global", 1.00, 2.00, 3.00, 4.00)
feature4 = st.slider("Move together - Trabajo en equipo - Global", 1.00, 2.00, 3.00, 4.00)
feature5 = st.slider("Speak up - Hablo abiertamente - Global", 1.00, 2.00, 3.00, 4.00)


# Create a button to perform the prediction
if st.button("Predict"):
    # Prepare the input data as a DataFrame
    input_data = pd.DataFrame({
        "Think Customer": [feature1],
        "Embrace Change": [feature2],
        "Act Now": [feature3],
        "Move together": [feature4],
        "Speak up": [feature5],
        "Spain of Control": [feature6]
    })

    # Make the prediction using the loaded model
    prediction = model.predict(input_data)[0]

    # Display the prediction
    st.write(f"Valoracion 360 Global: {prediction:.2f}")